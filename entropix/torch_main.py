from typing import NamedTuple, Optional, Tuple

import torch
import torch.nn.functional as F

import math
import tyro

from pathlib import Path
from functools import partial

from entropix.config import LLAMA_1B_PARAMS
from entropix.tokenizer import Tokenizer
from entropix.torch_kvcache import KVCache
from entropix.torch_model import xfmr
from entropix.torch_weights import XfmrWeights, LayerWeights, load_weights
from entropix.torch_sampler import sample, SamplerConfig
from entropix.prompts import create_prompts_from_csv, prompt

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# Device selection, tree is like first apple silicion, then cuda, fallback is cpu.
if torch.backends.mps.is_available():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

print(f"Using device: {device}")

torch.set_float32_matmul_precision('high')

def apply_scaling(freqs: torch.Tensor) -> torch.Tensor:
    SCALE_FACTOR = 8.0
    LOW_FREQ_FACTOR = 1.0
    HIGH_FREQ_FACTOR = 4.0
    OLD_CONTEXT_LEN = 8192  # original llama3 length

    low_freq_wavelen = OLD_CONTEXT_LEN / LOW_FREQ_FACTOR
    high_freq_wavelen = OLD_CONTEXT_LEN / HIGH_FREQ_FACTOR

    def scale_freq(freq: torch.Tensor) -> torch.Tensor:
        wavelen = 2 * torch.pi / freq

        # Calculate smooth factor
        smooth = (OLD_CONTEXT_LEN / wavelen - LOW_FREQ_FACTOR) / (HIGH_FREQ_FACTOR - LOW_FREQ_FACTOR)
        smooth = torch.clamp(smooth, 0.0, 1.0)  # Ensure smooth is between 0 and 1

        # Calculate scaled frequency
        scaled = (1 - smooth) * freq / SCALE_FACTOR + smooth * freq

        # Apply conditional scaling
        scaled = torch.where(
            wavelen < high_freq_wavelen,
            freq,  # No scaling
            torch.where(
                wavelen > low_freq_wavelen,
                freq / SCALE_FACTOR,  # Apply scaling factor
                scaled  # Apply smooth scaling
            )
        )
        return scaled

    scaled_freqs = torch.vmap(scale_freq)(freqs)
    
    return scaled_freqs

def precompute_freqs_cis(dim: int, end: int, theta: float = 500000.0, use_scaled: bool = False, dtype: torch.dtype = torch.float32) -> torch.Tensor:
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=dtype, device=device)[: (dim // 2)] / dim))
    if use_scaled:
        freqs = apply_scaling(freqs)

    t = torch.arange(end, dtype=dtype, device=device).unsqueeze(1)  # Shape: (end, 1)
    freqs = freqs.unsqueeze(0)  # Shape: (1, dim//2)
    freqs = t * freqs  # Broadcasting to shape: (end, dim//2)
    return torch.exp(1j * freqs)



def build_attn_mask(seqlen: int, start_pos: int) -> torch.Tensor:
  mask = torch.zeros((seqlen, seqlen), dtype=torch.float32).to(device)
  if seqlen > 1:
      mask = torch.full((seqlen, seqlen), float("-inf"))
      mask = torch.triu(mask, diagonal=1)
      mask = torch.hstack([torch.zeros((seqlen, start_pos)), mask]).to(torch.float32).to(device)
  return mask

def rgb_to_ansi(r: int, g: int, b: int) -> str:
    """Convert RGB color to ANSI escape sequence."""
    return f"\033[38;2;{r};{g};{b}m"

def apply_color_and_format(text: str, color: Tuple[int, int, int], formatting: str) -> str:
    """Apply color and formatting to text."""
    color_code = rgb_to_ansi(*color)
    return f"{color_code}{formatting}{text}\033[0m"

def print_colored(text: str, color: Tuple[int, int, int], formatting: str, end: str = ''):
    """Print text with color and formatting."""
    colored_text = apply_color_and_format(text, color, formatting)
    print(colored_text, end=end, flush=True)


def main():
  with torch.inference_mode():
    model_params = LLAMA_1B_PARAMS
    xfmr_weights = load_weights()

    tokenizer = Tokenizer('entropix/tokenizer.model')
    raw_tokens1 = tokenizer.encode(prompt,  bos=False, eos=False, allowed_special='all')
    #this is not used in this script, but can be used to generate base_raw_tokens1
    # base_raw_tokens1 = tokenizer.encode(bp1, bos=True, eos=False, allowed_special='all')


    def generate(xfmr_weights, model_params, tokens):
      gen_tokens = None
      cur_pos = 0
      tokens = torch.tensor([tokens], dtype=torch.long).to(device)
      bsz, seqlen = tokens.shape
      attn_mask = build_attn_mask(seqlen, cur_pos)
      freqs_cis = precompute_freqs_cis(model_params.head_dim, model_params.max_seq_len, model_params.rope_theta, model_params.use_scaled_rope)
      kvcache = KVCache.new(model_params.n_layers, bsz, model_params.max_seq_len, model_params.n_local_kv_heads, model_params.head_dim).to(DEVICE)
      logits, kvcache, _, _ = xfmr(xfmr_weights, model_params, tokens, cur_pos, freqs_cis[:seqlen], kvcache, attn_mask=attn_mask)
      next_token = torch.argmax(logits[:, -1], dim=-1, keepdim=True).to(torch.int32)
      gen_tokens = next_token
      print(tokenizer.decode([next_token.item()]), end='', flush=True)
      cur_pos = seqlen
      stop = torch.tensor([128001, 128008, 128009], device=device, dtype=torch.int32)
      sampler_cfg = SamplerConfig()
      while cur_pos < 8192:
        cur_pos += 1
        logits, kvcache, scores, stats = xfmr(xfmr_weights, model_params, next_token, cur_pos, freqs_cis[cur_pos:cur_pos+1], kvcache)
        next_token, color = sample(gen_tokens, logits, scores, cfg = sampler_cfg)
        gen_tokens = torch.cat((gen_tokens, next_token), dim=1)
        out_token = tokenizer.decode(next_token.tolist()[0])
        print_colored(out_token, color[0], color[1], end='')
        if torch.isin(next_token, stop).any():
          break

    csv_path = Path('entropix/data/acp.csv')
    prompts = create_prompts_from_csv(csv_path)
    PROMPT_TEST = False

    if PROMPT_TEST:
      for test_prompt in prompts:
        print(test_prompt)
        tokens = tokenizer.encode(test_prompt, bos=False, eos=False, allowed_special='all')
        generate(xfmr_weights, model_params, tokens)
        print()
    else:
        raw_tokens1 = tokenizer.encode(prompt,  bos=False, eos=False, allowed_special='all')
        #this is not used in this script, but can be used to generate base_raw_tokens1
        # base_raw_tokens1 = tokenizer.encode(bp1, bos=True, eos=False, allowed_special='all')
        print(prompt)
        generate(xfmr_weights, model_params, raw_tokens1)
        print()

if __name__ == '__main__':
  tyro.cli(main)